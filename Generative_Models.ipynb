{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from ps4_utils import load_data,load_experiment\n",
    "from ps4_utils import AbstractGenerativeModel\n",
    "from ps4_utils import save_submission\n",
    "#from scipy.misc import logsumexp\n",
    "from scipy.special import logsumexp\n",
    "import numpy as np\n",
    "import random\n",
    "data_fn = \"datasets-ps4.h5\"\n",
    "MAX_OUTER_ITER = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtureModel(AbstractGenerativeModel):\n",
    "    def __init__(self, CLASSES, NUM_FEATURES, NUM_MIXTURE_COMPONENTS, MAX_ITER=50, EPS=10**(-5)):\n",
    "        AbstractGenerativeModel.__init__(self, CLASSES, NUM_FEATURES)\n",
    "        self.num_mixture_components = NUM_MIXTURE_COMPONENTS # list of num_mixture_components (length num_classes)\n",
    "        self.max_iter = MAX_ITER # max iterations of EM\n",
    "        self.epsilon = EPS # help with stability, to be used according to hint given at end of pset4.pdf\n",
    "        self.params = { # lists of length CLASSES\n",
    "            'pi': [np.repeat(1/k,k) for k in self.num_mixture_components], # with pi_c for each class\n",
    "            'theta': [np.zeros((self.num_features,k)) for k in self.num_mixture_components], # with theta_c for each class\n",
    "        }\n",
    "    def pack_params(self, X, class_idx):\n",
    "        pi,theta = self.fit(X[class_idx],class_idx) # fit parameters\n",
    "        self.params['pi'][class_idx] = pi # update member variable pi\n",
    "        self.params['theta'][class_idx] = theta #update member variable theta\n",
    "        \n",
    "    #make classification based on which mixture model gives higher probability to generating point xi\n",
    "    def classify(self, X):\n",
    "        P = list()\n",
    "        pi = self.params['pi']\n",
    "        theta = self.params['theta']\n",
    "        for c in range(self.num_classes):\n",
    "            _,Pc = self.findP(X, pi[c], theta[c])\n",
    "            P.append(Pc)\n",
    "        return np.vstack(P).T.argmax(-1) # np.array of class predictions for each data point in X\n",
    "\n",
    "    # --- E-step\n",
    "    def updateLatentPosterior(self, X, pi, theta, num_mixture_components): # update the latent posterior\n",
    "        # --- gamma: responsibilities (probabilities), np.array (matrix)\n",
    "        # ---        shape: number of data points in X (where X consists of datapoints from class c) by NUM_MIXTURE_COMPONENTS[c]\n",
    "        # note: can use output of findP here (with care taken to return gamma containing proper probabilities)\n",
    "        gamma = np.zeros((X.shape[0],num_mixture_components))\n",
    "        t,total = self.findP(X,pi,theta)\n",
    "        for c in range(num_mixture_components):\n",
    "            gamma[:,c] = np.exp(t[:,c]-total)\n",
    "        return gamma\n",
    "    \n",
    "    # --- M-step (1)\n",
    "    @staticmethod\n",
    "    def updatePi(gamma): #update the pi component using the posteriors (gammas)\n",
    "        # --- pi_c: class specific pi, np.array (vector)\n",
    "        # ---        shape: NUM_MIXTURE_COMPONENTS[c]\n",
    "        pi_c = np.sum(gamma,axis = 0)/gamma.shape[0]\n",
    "        return pi_c\n",
    "    \n",
    "    # -- M-step (2)\n",
    "    @staticmethod\n",
    "    def updateTheta(X, gamma): #update theta component using posteriors (gammas)\n",
    "        # --- theta_c: class specific theta, np.array matrix\n",
    "        # ---        shape: NUM_FEATURES by NUM_MIXTURE_COMPONENTS[c]\n",
    "        theta_c = np.dot(X.T,gamma)/np.sum(gamma,axis = 0)\n",
    "        return theta_c \n",
    "    \n",
    "    @staticmethod\n",
    "    def findP(X, pi, theta):\n",
    "        # --- t: logprobabilities of x given each component of mixture\n",
    "        # ---        shape: number of data points in X (where X consists of datapoints from class c) by NUM_MIXTURE_COMPONENTS[c] \n",
    "        # --- logsumexp(t,axis=1): (for convenience) once exponentiated, gives normalization factor over all mixture components\n",
    "        # ---        shape: number of data points in X (where X consists of datapoints from class c)\n",
    "        \n",
    "        new_theta = theta + 10**(-7)\n",
    "        t = np.log(pi) + np.dot(X,np.log(new_theta)) + np.dot((1-X),np.log(1-new_theta))\n",
    "        return t,logsumexp(t,axis=1)\n",
    "        \n",
    "    # --- execute EM procedure\n",
    "    def fit(self, X, class_idx):\n",
    "        max_iter = self.max_iter\n",
    "        eps = self.epsilon\n",
    "        N = X.shape[0]\n",
    "        pi = self.params['pi'][class_idx]\n",
    "        theta = self.params['theta'][class_idx]\n",
    "        num_mixture_components = self.num_mixture_components[class_idx]\n",
    "        # INITIALIZE theta, note theta is currently set to zeros but needs to be officially initialized here\n",
    "        for i in range(num_mixture_components):\n",
    "            a = np.random.randint(N, size= N//num_mixture_components)\n",
    "            theta[:,i] = np.mean(X[a,:],axis=0)\n",
    "        for i in range(max_iter):\n",
    "            # E-step: gamma = self.updateLatentPosterior\n",
    "            # M-step(1): pi = self.updatePi \n",
    "            # M-step(2): theta = self.updateTheta\n",
    "            theta[theta<eps] = eps\n",
    "            theta[theta>1-eps] = 1-eps\n",
    "            gamma = self.updateLatentPosterior(X, pi, theta, num_mixture_components)\n",
    "            pi = self.updatePi(gamma)\n",
    "            theta = self.updateTheta(X, gamma)\n",
    "        theta[theta<eps] = eps\n",
    "        theta[theta>1-eps] = 1-eps\n",
    "        return pi,theta #pi and theta, given class_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesModel(AbstractGenerativeModel):\n",
    "    def __init__(self, CLASSES, NUM_FEATURES, EPS=10**(-12)):\n",
    "        AbstractGenerativeModel.__init__(self, CLASSES, NUM_FEATURES)\n",
    "        self.epsilon = EPS # help with stability\n",
    "        self.params = {\n",
    "            'p': [np.zeros((NUM_FEATURES))] * self.num_classes # estimated log-probabilities of features for each class\n",
    "        }\n",
    "    def pack_params(self, X, class_idx):\n",
    "        p = self.fit(X[class_idx])\n",
    "        self.params['p'][class_idx] = p\n",
    "    def classify(self, X): # naive bayes classifier\n",
    "        # --- predictions: predictions for data points in X (where X consists of datapoints from class c), np.array (vector)\n",
    "        # ---        shape: number of data points\n",
    "        array = np.zeros((X.shape[0],len(self.params['p'])))\n",
    "        for i in range(X.shape[0]):\n",
    "            x = X[i]\n",
    "            for c in range(len(self.params['p'])): \n",
    "                prob = self.params['p'][c]+self.epsilon #theta_c\n",
    "                array[i][c] = np.sum(x*np.log(prob) + (1-x)*np.log(1-prob))\n",
    "        predictions = np.argmax(array,axis = 1)\n",
    "        return predictions\n",
    "    def fit(self, X):\n",
    "        # --- estimated_p: estimated p's of features for input X (where X consists of datapoints from class c), np.array (vector)\n",
    "        # ---        shape: NUM_FEATURES\n",
    "        estimated_p = np.sum(X,axis = 0)/X.shape[0]\n",
    "        return estimated_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTIMENT ANALYSIS -- NAIVE BAYES MODEL:\n",
      "ACCURACY ON VALIDATION: 0.74\n",
      "\n",
      "The confusion matrix of the validation \n",
      " dataset by running sentiment_analysis on naive bayes model\n",
      "[[ 91  56]\n",
      " [ 74 279]]\n",
      "\n",
      "SENTIMENT ANALYSIS -- MIXTURE MODEL:\n",
      "COMPONENTS: 8 4\n",
      "ACCURACY ON VALIDATION: 0.728\n",
      "COMPONENTS: 3 14\n",
      "ACCURACY ON VALIDATION: 0.658\n",
      "COMPONENTS: 11 13\n",
      "ACCURACY ON VALIDATION: 0.72\n",
      "COMPONENTS: 9 11\n",
      "ACCURACY ON VALIDATION: 0.726\n",
      "COMPONENTS: 6 9\n",
      "ACCURACY ON VALIDATION: 0.702\n",
      "COMPONENTS: 5 6\n",
      "ACCURACY ON VALIDATION: 0.712\n",
      "COMPONENTS: 14 5\n",
      "ACCURACY ON VALIDATION: 0.694\n",
      "COMPONENTS: 7 13\n",
      "ACCURACY ON VALIDATION: 0.71\n",
      "COMPONENTS: 10 5\n",
      "ACCURACY ON VALIDATION: 0.7\n",
      "COMPONENTS: 12 4\n",
      "ACCURACY ON VALIDATION: 0.728\n",
      "COMPONENTS: 3 7\n",
      "ACCURACY ON VALIDATION: 0.708\n",
      "COMPONENTS: 4 13\n",
      "ACCURACY ON VALIDATION: 0.678\n",
      "COMPONENTS: 10 12\n",
      "ACCURACY ON VALIDATION: 0.698\n",
      "COMPONENTS: 14 13\n",
      "ACCURACY ON VALIDATION: 0.72\n",
      "COMPONENTS: 14 8\n",
      "ACCURACY ON VALIDATION: 0.694\n",
      "COMPONENTS: 10 7\n",
      "ACCURACY ON VALIDATION: 0.732\n",
      "COMPONENTS: 14 4\n",
      "ACCURACY ON VALIDATION: 0.71\n",
      "COMPONENTS: 14 13\n",
      "ACCURACY ON VALIDATION: 0.716\n",
      "COMPONENTS: 11 6\n",
      "ACCURACY ON VALIDATION: 0.71\n",
      "COMPONENTS: 7 9\n",
      "ACCURACY ON VALIDATION: 0.724\n",
      "COMPONENTS: 7 7\n",
      "ACCURACY ON VALIDATION: 0.736\n",
      "COMPONENTS: 10 7\n",
      "ACCURACY ON VALIDATION: 0.72\n",
      "COMPONENTS: 4 4\n",
      "ACCURACY ON VALIDATION: 0.75\n",
      "COMPONENTS: 3 13\n",
      "ACCURACY ON VALIDATION: 0.664\n",
      "COMPONENTS: 7 11\n",
      "ACCURACY ON VALIDATION: 0.702\n",
      "COMPONENTS: 9 3\n",
      "ACCURACY ON VALIDATION: 0.708\n",
      "COMPONENTS: 8 9\n",
      "ACCURACY ON VALIDATION: 0.694\n",
      "COMPONENTS: 6 13\n",
      "ACCURACY ON VALIDATION: 0.702\n",
      "COMPONENTS: 11 5\n",
      "ACCURACY ON VALIDATION: 0.69\n",
      "COMPONENTS: 13 4\n",
      "ACCURACY ON VALIDATION: 0.704\n",
      "COMPONENTS: 14 9\n",
      "ACCURACY ON VALIDATION: 0.708\n",
      "COMPONENTS: 7 2\n",
      "ACCURACY ON VALIDATION: 0.704\n",
      "COMPONENTS: 3 14\n",
      "ACCURACY ON VALIDATION: 0.648\n",
      "COMPONENTS: 7 7\n",
      "ACCURACY ON VALIDATION: 0.72\n",
      "COMPONENTS: 13 4\n",
      "ACCURACY ON VALIDATION: 0.7\n",
      "COMPONENTS: 4 11\n",
      "ACCURACY ON VALIDATION: 0.692\n",
      "COMPONENTS: 11 8\n",
      "ACCURACY ON VALIDATION: 0.746\n",
      "COMPONENTS: 4 2\n",
      "ACCURACY ON VALIDATION: 0.738\n",
      "COMPONENTS: 3 14\n",
      "ACCURACY ON VALIDATION: 0.664\n",
      "COMPONENTS: 7 5\n",
      "ACCURACY ON VALIDATION: 0.72\n",
      "COMPONENTS: 10 9\n",
      "ACCURACY ON VALIDATION: 0.736\n",
      "COMPONENTS: 3 8\n",
      "ACCURACY ON VALIDATION: 0.688\n",
      "COMPONENTS: 2 2\n",
      "ACCURACY ON VALIDATION: 0.734\n",
      "COMPONENTS: 14 10\n",
      "ACCURACY ON VALIDATION: 0.706\n",
      "COMPONENTS: 5 2\n",
      "ACCURACY ON VALIDATION: 0.728\n",
      "COMPONENTS: 12 10\n",
      "ACCURACY ON VALIDATION: 0.688\n",
      "COMPONENTS: 7 8\n",
      "ACCURACY ON VALIDATION: 0.732\n",
      "COMPONENTS: 4 2\n",
      "ACCURACY ON VALIDATION: 0.712\n",
      "COMPONENTS: 3 11\n",
      "ACCURACY ON VALIDATION: 0.688\n",
      "COMPONENTS: 14 6\n",
      "ACCURACY ON VALIDATION: 0.722\n",
      "Saved: mm-sentiment_analysis-submission.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "experiment_name = \"sentiment_analysis\"\n",
    "# --- SENTIMENT ANALYSIS setup\n",
    "Xtrain,Xval,num_classes,num_features = load_experiment(data_fn, experiment_name)\n",
    "# -- build naive bayes model for sentiment analysis\n",
    "print(\"SENTIMENT ANALYSIS -- NAIVE BAYES MODEL:\")\n",
    "nbm = NaiveBayesModel(num_classes, num_features)\n",
    "nbm.train(Xtrain)\n",
    "print(\"ACCURACY ON VALIDATION: \" + str(nbm.val(Xval)))\n",
    "array1 =[]\n",
    "array2 =[]\n",
    "\n",
    "for i in range(num_classes):\n",
    "    array1.append(nbm.classify(Xval[i]))\n",
    "    array2.append(np.array([i]*len(nbm.classify(Xval[i]))))\n",
    "y_pred = np.hstack(array1)\n",
    "y_true = np.hstack(array2)\n",
    "print() \n",
    "print('The confusion matrix of the validation \\n dataset by running sentiment_analysis on naive bayes model')\n",
    "print(metrics.confusion_matrix(y_pred,y_true))\n",
    "print()\n",
    "\n",
    "# -- build mixture model for sentiment analysis\n",
    "print(\"SENTIMENT ANALYSIS -- MIXTURE MODEL:\")\n",
    "for i in range(MAX_OUTER_ITER):\n",
    "    num_mixture_components =  np.random.randint(2,15,num_classes)\n",
    "    print(\"COMPONENTS: \" + \" \".join(str(i) for i in num_mixture_components))\n",
    "    mm = MixtureModel(num_classes, num_features, num_mixture_components)\n",
    "    mm.train(Xtrain)\n",
    "    print(\"ACCURACY ON VALIDATION: \" + str(mm.val(Xval)))\n",
    "\n",
    "# submit to kaggle\n",
    "Xkaggle = load_data(data_fn, experiment_name, \"kaggle\")\n",
    "save_submission(\"mm-{}-submission.csv\".format(experiment_name), mm.classify(Xkaggle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST DIGIT CLASSIFICATION -- NAIVE BAYES MODEL:\n",
      "ACCURACY ON VALIDATION: 0.7355\n",
      "\n",
      "The confusion matrix of the validation \n",
      " dataset by running mnist on naive bayes model\n",
      "[[151   0   5   3   2   7   2   1   2   1]\n",
      " [  0 206  14   7   3  12   8  13  18   5]\n",
      " [  5   5 147  10   5   3   6   2  13   3]\n",
      " [  5   1  11 137   0  31   2   3  20  10]\n",
      " [  0   2   6   0 147   9   9  12   5  22]\n",
      " [ 10   3   2   8   4  91   9   1   8   5]\n",
      " [ 11   1   9   2   2   5 177   0   3   1]\n",
      " [  1   2   3   4   6   6   1 179   0  21]\n",
      " [  2   2   9   8   3   6   2   2 122   4]\n",
      " [  2   1   4   4  21   2   1  20   5 114]]\n",
      "\n",
      "MNIST DIGIT CLASSIFICATION -- MIXTURE MODEL:\n",
      "COMPONENTS: 3 12 14 5 12 11 12 2 7 11\n",
      "ACCURACY ON VALIDATION: 0.7625\n",
      "COMPONENTS: 10 6 8 2 12 10 11 12 6 7\n",
      "ACCURACY ON VALIDATION: 0.7655\n",
      "COMPONENTS: 14 4 8 8 6 3 7 13 6 3\n",
      "ACCURACY ON VALIDATION: 0.7685\n",
      "COMPONENTS: 2 11 5 11 9 13 8 14 9 10\n",
      "ACCURACY ON VALIDATION: 0.772\n",
      "COMPONENTS: 8 11 11 10 13 8 4 12 7 2\n",
      "ACCURACY ON VALIDATION: 0.763\n",
      "COMPONENTS: 14 5 6 10 6 14 12 3 2 11\n",
      "ACCURACY ON VALIDATION: 0.771\n",
      "COMPONENTS: 5 14 7 13 8 5 2 2 4 11\n",
      "ACCURACY ON VALIDATION: 0.774\n",
      "COMPONENTS: 12 4 4 13 7 3 13 6 6 5\n",
      "ACCURACY ON VALIDATION: 0.7715\n",
      "COMPONENTS: 8 13 14 7 4 10 10 8 7 6\n",
      "ACCURACY ON VALIDATION: 0.7735\n",
      "COMPONENTS: 4 6 11 11 10 11 12 13 10 9\n",
      "ACCURACY ON VALIDATION: 0.7695\n",
      "COMPONENTS: 13 11 11 7 6 9 3 11 11 5\n",
      "ACCURACY ON VALIDATION: 0.7755\n",
      "COMPONENTS: 12 9 5 2 14 5 12 2 7 6\n",
      "ACCURACY ON VALIDATION: 0.7625\n",
      "COMPONENTS: 2 11 7 6 14 13 5 6 8 2\n",
      "ACCURACY ON VALIDATION: 0.762\n",
      "COMPONENTS: 8 2 11 7 6 9 10 13 6 2\n",
      "ACCURACY ON VALIDATION: 0.7635\n",
      "COMPONENTS: 4 7 3 8 7 8 14 6 2 13\n",
      "ACCURACY ON VALIDATION: 0.773\n",
      "COMPONENTS: 11 4 11 11 8 10 11 8 3 7\n",
      "ACCURACY ON VALIDATION: 0.777\n",
      "COMPONENTS: 12 7 8 13 11 9 2 9 14 13\n",
      "ACCURACY ON VALIDATION: 0.774\n",
      "COMPONENTS: 14 12 11 14 6 2 3 13 6 13\n",
      "ACCURACY ON VALIDATION: 0.7715\n",
      "COMPONENTS: 4 2 14 6 4 2 12 8 4 14\n",
      "ACCURACY ON VALIDATION: 0.767\n",
      "COMPONENTS: 5 2 3 11 10 3 4 14 6 6\n",
      "ACCURACY ON VALIDATION: 0.78\n",
      "COMPONENTS: 6 7 3 13 2 8 2 14 6 10\n",
      "ACCURACY ON VALIDATION: 0.7775\n",
      "COMPONENTS: 6 4 9 4 3 9 7 8 5 10\n",
      "ACCURACY ON VALIDATION: 0.786\n",
      "COMPONENTS: 3 2 9 5 14 7 6 3 5 4\n",
      "ACCURACY ON VALIDATION: 0.776\n",
      "COMPONENTS: 14 4 3 14 2 8 7 5 2 5\n",
      "ACCURACY ON VALIDATION: 0.779\n",
      "COMPONENTS: 13 7 14 5 7 13 7 2 14 7\n",
      "ACCURACY ON VALIDATION: 0.7625\n",
      "COMPONENTS: 6 8 12 4 8 8 10 6 5 3\n",
      "ACCURACY ON VALIDATION: 0.7755\n",
      "COMPONENTS: 7 4 9 9 3 14 7 10 10 4\n",
      "ACCURACY ON VALIDATION: 0.7765\n",
      "COMPONENTS: 13 5 11 8 12 12 4 4 3 13\n",
      "ACCURACY ON VALIDATION: 0.775\n",
      "COMPONENTS: 13 10 9 10 11 14 11 9 11 12\n",
      "ACCURACY ON VALIDATION: 0.7625\n",
      "COMPONENTS: 7 2 4 11 14 8 5 9 12 14\n",
      "ACCURACY ON VALIDATION: 0.771\n",
      "COMPONENTS: 13 14 2 6 12 7 13 11 7 11\n",
      "ACCURACY ON VALIDATION: 0.7675\n",
      "COMPONENTS: 10 5 2 8 14 12 6 2 13 14\n",
      "ACCURACY ON VALIDATION: 0.765\n",
      "COMPONENTS: 2 3 5 10 12 5 14 6 10 13\n",
      "ACCURACY ON VALIDATION: 0.7715\n",
      "COMPONENTS: 8 6 7 4 9 6 13 11 12 11\n",
      "ACCURACY ON VALIDATION: 0.7655\n",
      "COMPONENTS: 5 13 3 13 13 12 14 5 12 9\n",
      "ACCURACY ON VALIDATION: 0.7635\n",
      "COMPONENTS: 3 8 4 14 3 12 3 7 9 2\n",
      "ACCURACY ON VALIDATION: 0.766\n",
      "COMPONENTS: 6 10 3 14 7 13 2 7 13 7\n",
      "ACCURACY ON VALIDATION: 0.7745\n",
      "COMPONENTS: 14 7 3 13 5 5 9 6 9 14\n",
      "ACCURACY ON VALIDATION: 0.773\n",
      "COMPONENTS: 11 9 11 5 6 6 14 3 6 7\n",
      "ACCURACY ON VALIDATION: 0.779\n",
      "COMPONENTS: 12 13 7 10 5 11 9 10 3 4\n",
      "ACCURACY ON VALIDATION: 0.773\n",
      "COMPONENTS: 9 12 9 9 8 11 6 6 11 8\n",
      "ACCURACY ON VALIDATION: 0.7795\n",
      "COMPONENTS: 9 3 11 9 2 3 2 12 13 12\n",
      "ACCURACY ON VALIDATION: 0.7675\n",
      "COMPONENTS: 11 11 4 3 12 3 13 14 7 6\n",
      "ACCURACY ON VALIDATION: 0.7705\n",
      "COMPONENTS: 13 13 4 11 9 12 8 5 6 11\n",
      "ACCURACY ON VALIDATION: 0.7835\n",
      "COMPONENTS: 6 7 9 11 2 2 10 12 3 9\n",
      "ACCURACY ON VALIDATION: 0.7685\n",
      "COMPONENTS: 11 2 5 4 12 10 14 7 12 2\n",
      "ACCURACY ON VALIDATION: 0.762\n",
      "COMPONENTS: 5 7 5 6 12 10 6 13 8 14\n",
      "ACCURACY ON VALIDATION: 0.7765\n",
      "COMPONENTS: 6 2 6 14 9 6 3 12 6 6\n",
      "ACCURACY ON VALIDATION: 0.7825\n",
      "COMPONENTS: 11 10 4 10 14 6 8 4 2 14\n",
      "ACCURACY ON VALIDATION: 0.778\n",
      "COMPONENTS: 7 11 3 7 5 6 2 11 5 9\n",
      "ACCURACY ON VALIDATION: 0.7885\n",
      "Saved: mm-mnist-submission.csv\n"
     ]
    }
   ],
   "source": [
    "experiment_name = \"mnist\"\n",
    "# --- MNIST DIGIT CLASSIFICATION setup\n",
    "Xtrain,Xval,num_classes,num_features = load_experiment(data_fn, experiment_name)\n",
    "# -- build naive bayes model for mnist digit classification\n",
    "print(\"MNIST DIGIT CLASSIFICATION -- NAIVE BAYES MODEL:\")\n",
    "nbm = NaiveBayesModel(num_classes, num_features)\n",
    "nbm.train(Xtrain)\n",
    "print(\"ACCURACY ON VALIDATION: \" + str(nbm.val(Xval)))\n",
    "\n",
    "array1 =[]\n",
    "array2 =[]\n",
    "\n",
    "for i in range(num_classes):\n",
    "    array1.append(nbm.classify(Xval[i]))\n",
    "    array2.append(np.array([i]*len(nbm.classify(Xval[i]))))\n",
    "y_pred = np.hstack(array1)\n",
    "y_true = np.hstack(array2)\n",
    "print()\n",
    "print('The confusion matrix of the validation \\n dataset by running mnist on naive bayes model')\n",
    "print(metrics.confusion_matrix(y_pred,y_true))\n",
    "print()\n",
    "\n",
    "# -- build mixture model for mnist digit classification\n",
    "print(\"MNIST DIGIT CLASSIFICATION -- MIXTURE MODEL:\")\n",
    "for i in range(MAX_OUTER_ITER):\n",
    "    num_mixture_components =  np.random.randint(2,15,num_classes)\n",
    "    print(\"COMPONENTS: \" + \" \".join(str(i) for i in num_mixture_components))\n",
    "    mm = MixtureModel(num_classes, num_features, num_mixture_components)\n",
    "    mm.train(Xtrain)\n",
    "    print(\"ACCURACY ON VALIDATION: \" + str(mm.val(Xval)))\n",
    "    \n",
    "# submit to kaggle\n",
    "Xkaggle = load_data(data_fn, experiment_name, \"kaggle\")\n",
    "save_submission(\"mm-{}-submission.csv\".format(experiment_name), mm.classify(Xkaggle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
